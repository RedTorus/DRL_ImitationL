  0%|                                                                                         | 0/50000 [00:00<?, ?it/s]/home/kaust/anaconda3/envs/drl/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Training Iteration 0: loss = 3.6311702728271484
 16%|████████████▊                                                                 | 8191/50000 [02:26<12:24, 56.13it/s]
Training Iteration 256: loss = 0.2977002263069153
Training Iteration 512: loss = 0.20899534225463867
Training Iteration 768: loss = 0.1843319982290268
Training Iteration 1024: loss = 0.16316187381744385
Training Iteration 1280: loss = 0.13156414031982422
Training Iteration 1536: loss = 0.12563475966453552
Training Iteration 1792: loss = 0.11125876754522324
Training Iteration 2048: loss = 0.1246442049741745
Training Iteration 2304: loss = 0.09986849874258041
Training Iteration 2560: loss = 0.12400543689727783
Training Iteration 2816: loss = 0.09752114862203598
Training Iteration 3072: loss = 0.12247117608785629
Training Iteration 3328: loss = 0.08582364022731781
Training Iteration 3584: loss = 0.12107455730438232
Training Iteration 3840: loss = 0.08861470222473145
Training Iteration 4096: loss = 0.0910031646490097
Training Iteration 4352: loss = 0.0839899405837059
Training Iteration 4608: loss = 0.09709441661834717
Training Iteration 4864: loss = 0.0782238095998764
Training Iteration 5120: loss = 0.09736615419387817
Training Iteration 5376: loss = 0.07463690638542175
Training Iteration 5632: loss = 0.07655587792396545
Training Iteration 5888: loss = 0.07352262735366821
Training Iteration 6144: loss = 0.08821036666631699
Training Iteration 6400: loss = 0.06684134155511856
Training Iteration 6656: loss = 0.07782289385795593
Training Iteration 6912: loss = 0.08355902135372162
Training Iteration 7168: loss = 0.08938893675804138
Training Iteration 7424: loss = 0.07824745774269104
Training Iteration 7680: loss = 0.07115963846445084
Training Iteration 7936: loss = 0.09139113128185272
Training Iteration 8192: loss = 0.07372289896011353
Training Iteration 8448: loss = 0.07063758373260498
Training Iteration 8704: loss = 0.06067871302366257
Training Iteration 8960: loss = 0.06599610298871994
Training Iteration 9216: loss = 0.060626041144132614
Training Iteration 9472: loss = 0.06825566291809082
Training Iteration 9728: loss = 0.05646055191755295
Training Iteration 9984: loss = 0.050783030688762665
Training Iteration 10240: loss = 0.06102187559008598
Training Iteration 10496: loss = 0.07521236687898636
Training Iteration 10752: loss = 0.05847683548927307
Training Iteration 11008: loss = 0.07349568605422974
Training Iteration 11264: loss = 0.07102226465940475
Training Iteration 11520: loss = 0.06849506497383118
Training Iteration 11776: loss = 0.07574295997619629
Training Iteration 12032: loss = 0.057941630482673645
Training Iteration 12288: loss = 0.06362447142601013
Training Iteration 12544: loss = 0.057380311191082
Training Iteration 12800: loss = 0.05526385456323624
Training Iteration 13056: loss = 0.050536077469587326
Training Iteration 13312: loss = 0.056079309433698654
Training Iteration 13568: loss = 0.06397110223770142
Training Iteration 13824: loss = 0.05620391294360161
Training Iteration 14080: loss = 0.053140781819820404
Training Iteration 14336: loss = 0.0576956681907177
Training Iteration 14592: loss = 0.0657702386379242
Training Iteration 14848: loss = 0.06962364912033081
Training Iteration 15104: loss = 0.04572979360818863
Training Iteration 15360: loss = 0.06316892802715302
Training Iteration 15616: loss = 0.06885415315628052
Training Iteration 15872: loss = 0.04715688154101372
Training Iteration 16128: loss = 0.06968307495117188
Training Iteration 16384: loss = 0.05446584150195122
Training Iteration 16640: loss = 0.05461933836340904
Training Iteration 16896: loss = 0.06005793809890747
Training Iteration 17152: loss = 0.04258370399475098
Training Iteration 17408: loss = 0.055374737828969955
Training Iteration 17664: loss = 0.06747499108314514
Training Iteration 17920: loss = 0.06896432489156723
Training Iteration 18176: loss = 0.05278025195002556
Training Iteration 18432: loss = 0.04735330864787102
Training Iteration 18688: loss = 0.05112766847014427
Training Iteration 18944: loss = 0.049741849303245544
Training Iteration 19200: loss = 0.04847393557429314
Training Iteration 19456: loss = 0.06191909313201904
Training Iteration 19712: loss = 0.04632514342665672
Training Iteration 19968: loss = 0.06184935197234154
Training Iteration 20224: loss = 0.07248072326183319
Training Iteration 20480: loss = 0.041445210576057434
Training Iteration 20736: loss = 0.06480695307254791
Training Iteration 20992: loss = 0.06296864151954651
Training Iteration 21248: loss = 0.06185580790042877
Training Iteration 21504: loss = 0.05577480047941208
Training Iteration 21760: loss = 0.05495811998844147
Training Iteration 22016: loss = 0.05206155776977539
Training Iteration 22272: loss = 0.06520355492830276
Training Iteration 22528: loss = 0.0431051179766655
Training Iteration 22784: loss = 0.06302346289157867
Training Iteration 23040: loss = 0.05248057097196579
Training Iteration 23296: loss = 0.060955654829740524
Training Iteration 23552: loss = 0.05262505263090134
Training Iteration 23808: loss = 0.04590943455696106
Training Iteration 24064: loss = 0.047212790697813034
Training Iteration 24320: loss = 0.05033838003873825
Training Iteration 24576: loss = 0.04830966517329216
Training Iteration 24832: loss = 0.06400647759437561
Training Iteration 25088: loss = 0.052815672010183334
Training Iteration 25344: loss = 0.05423334240913391
Training Iteration 25600: loss = 0.05846136063337326
Training Iteration 25856: loss = 0.057853519916534424
Training Iteration 26112: loss = 0.04830695316195488
Training Iteration 26368: loss = 0.0627780482172966
Training Iteration 26624: loss = 0.061714716255664825
Training Iteration 26880: loss = 0.037129756063222885
Training Iteration 27136: loss = 0.04921963810920715
Training Iteration 27392: loss = 0.051122091710567474
Training Iteration 27648: loss = 0.04359784722328186
Training Iteration 27904: loss = 0.049137458205223083
Training Iteration 28160: loss = 0.05069652199745178
Training Iteration 28416: loss = 0.05208800360560417
Training Iteration 28672: loss = 0.04808162897825241
Training Iteration 28928: loss = 0.0493704229593277
Training Iteration 29184: loss = 0.06163012981414795
Training Iteration 29440: loss = 0.043292101472616196
Training Iteration 29696: loss = 0.04782222956418991
Training Iteration 29952: loss = 0.047188274562358856
Training Iteration 30208: loss = 0.06583032011985779
Training Iteration 30464: loss = 0.04583555459976196
Training Iteration 30720: loss = 0.050884924829006195
Training Iteration 30976: loss = 0.07381689548492432
Training Iteration 31232: loss = 0.04559480398893356
Training Iteration 31488: loss = 0.045809030532836914
Training Iteration 31744: loss = 0.04776390641927719
Training Iteration 32000: loss = 0.05114857107400894
Training Iteration 32256: loss = 0.042655229568481445
Training Iteration 32512: loss = 0.038869112730026245
Training Iteration 32768: loss = 0.039149872958660126
Training Iteration 33024: loss = 0.043144792318344116
Training Iteration 33280: loss = 0.04889894649386406
Training Iteration 33536: loss = 0.05908995866775513
Training Iteration 33792: loss = 0.04922030493617058
Training Iteration 34048: loss = 0.04332810267806053
Training Iteration 34304: loss = 0.04575201869010925
Training Iteration 34560: loss = 0.04916972666978836
Training Iteration 34816: loss = 0.035308919847011566
Training Iteration 35072: loss = 0.036417193710803986
Training Iteration 35328: loss = 0.03277941420674324
Training Iteration 35584: loss = 0.045174188911914825
Training Iteration 35840: loss = 0.05068107694387436
Training Iteration 36096: loss = 0.033246178179979324
Training Iteration 36352: loss = 0.03876303881406784
Training Iteration 36608: loss = 0.050364166498184204
Training Iteration 36864: loss = 0.04280773550271988
Training Iteration 37120: loss = 0.06299041211605072
Training Iteration 37376: loss = 0.0418352410197258
Training Iteration 37632: loss = 0.049722325056791306
Training Iteration 37888: loss = 0.05933215469121933
Training Iteration 38144: loss = 0.042536091059446335
Training Iteration 38400: loss = 0.05723758786916733
Training Iteration 38656: loss = 0.03511481732130051
Training Iteration 38912: loss = 0.04076017439365387
Training Iteration 39168: loss = 0.0447448194026947
Training Iteration 39424: loss = 0.05270218849182129
Training Iteration 39680: loss = 0.04469088837504387
Training Iteration 39936: loss = 0.03846288472414017
Training Iteration 40192: loss = 0.05593465268611908
Training Iteration 40448: loss = 0.03942282125353813
Training Iteration 40704: loss = 0.04413677006959915
Training Iteration 40960: loss = 0.0647290050983429
Training Iteration 41216: loss = 0.044039275497198105
Training Iteration 41472: loss = 0.046945251524448395
Training Iteration 41728: loss = 0.04230450838804245
Training Iteration 41984: loss = 0.03210797905921936
Training Iteration 42240: loss = 0.04282594844698906
Training Iteration 42496: loss = 0.036507464945316315
Training Iteration 42752: loss = 0.04323732107877731
Training Iteration 43008: loss = 0.04064866155385971
Training Iteration 43264: loss = 0.06527577340602875
Training Iteration 43520: loss = 0.04986478015780449
Training Iteration 43776: loss = 0.036544471979141235
Training Iteration 44032: loss = 0.04048685356974602
Training Iteration 44288: loss = 0.04249519854784012
Training Iteration 44544: loss = 0.03790733590722084
Training Iteration 44800: loss = 0.04225728660821915
Training Iteration 45056: loss = 0.04629441350698471
Training Iteration 45312: loss = 0.03948131948709488
Training Iteration 45568: loss = 0.034673187881708145
Training Iteration 45824: loss = 0.042291056364774704
Training Iteration 46080: loss = 0.04061564803123474
Training Iteration 46336: loss = 0.033130474388599396
Training Iteration 46592: loss = 0.0430864542722702
Training Iteration 46848: loss = 0.04496971517801285
Training Iteration 47104: loss = 0.03837306797504425
Training Iteration 47360: loss = 0.04290103167295456
Training Iteration 47616: loss = 0.04320878908038139
Training Iteration 47872: loss = 0.04009018838405609
Training Iteration 48128: loss = 0.03662368282675743
Training Iteration 48384: loss = 0.046580635011196136
Training Iteration 48640: loss = 0.04110331833362579
Training Iteration 48896: loss = 0.0399467907845974
Training Iteration 49152: loss = 0.042572326958179474
Training Iteration 49408: loss = 0.03512382507324219
Training Iteration 49664: loss = 0.036395300179719925
Training Iteration 49920: loss = 0.046730488538742065
