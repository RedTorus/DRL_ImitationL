  0%|                                                                                                           | 0/50000 [00:00<?, ?it/s]/home/kaust/anaconda3/envs/drl/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Training Iteration 0: loss = 4.148610591888428
 16%|███████████████▋                                                                                | 8191/50000 [02:26<12:21, 56.40it/s]
Training Iteration 256: loss = 0.2985512614250183
Training Iteration 512: loss = 0.24810676276683807
Training Iteration 768: loss = 0.19854509830474854
Training Iteration 1024: loss = 0.13564151525497437
Training Iteration 1280: loss = 0.15021175146102905
Training Iteration 1536: loss = 0.12429021298885345
Training Iteration 1792: loss = 0.10742100328207016
Training Iteration 2048: loss = 0.11654351651668549
Training Iteration 2304: loss = 0.10594139993190765
Training Iteration 2560: loss = 0.1255095899105072
Training Iteration 2816: loss = 0.10131537914276123
Training Iteration 3072: loss = 0.09293162822723389
Training Iteration 3328: loss = 0.10075527429580688
Training Iteration 3584: loss = 0.07439243048429489
Training Iteration 3840: loss = 0.08168771117925644
Training Iteration 4096: loss = 0.08900212496519089
Training Iteration 4352: loss = 0.06797260046005249
Training Iteration 4608: loss = 0.09232302755117416
Training Iteration 4864: loss = 0.08284281194210052
Training Iteration 5120: loss = 0.0959119200706482
Training Iteration 5376: loss = 0.09371720254421234
Training Iteration 5632: loss = 0.06786836683750153
Training Iteration 5888: loss = 0.0798712968826294
Training Iteration 6144: loss = 0.08019684255123138
Training Iteration 6400: loss = 0.08678321540355682
Training Iteration 6656: loss = 0.081957146525383
Training Iteration 6912: loss = 0.07566981017589569
Training Iteration 7168: loss = 0.08752363920211792
Training Iteration 7424: loss = 0.06864160299301147
Training Iteration 7680: loss = 0.06670708954334259
Training Iteration 7936: loss = 0.06998459994792938
Training Iteration 8192: loss = 0.08544646203517914
Training Iteration 8448: loss = 0.05854880064725876
Training Iteration 8704: loss = 0.07093343883752823
Training Iteration 8960: loss = 0.06591492891311646
Training Iteration 9216: loss = 0.06349854916334152
Training Iteration 9472: loss = 0.06950603425502777
Training Iteration 9728: loss = 0.08124831318855286
Training Iteration 9984: loss = 0.0820402204990387
Training Iteration 10240: loss = 0.06756921112537384
Training Iteration 10496: loss = 0.06369902193546295
Training Iteration 10752: loss = 0.0580233633518219
Training Iteration 11008: loss = 0.0639384537935257
Training Iteration 11264: loss = 0.06086459010839462
Training Iteration 11520: loss = 0.05983991548418999
Training Iteration 11776: loss = 0.06307122111320496
Training Iteration 12032: loss = 0.064043790102005
Training Iteration 12288: loss = 0.06649039685726166
Training Iteration 12544: loss = 0.07089154422283173
Training Iteration 12800: loss = 0.05485448241233826
Training Iteration 13056: loss = 0.06510557979345322
Training Iteration 13312: loss = 0.06665921211242676
Training Iteration 13568: loss = 0.04835737869143486
Training Iteration 13824: loss = 0.05593128129839897
Training Iteration 14080: loss = 0.06276272982358932
Training Iteration 14336: loss = 0.06171520799398422
Training Iteration 14592: loss = 0.06113036721944809
Training Iteration 14848: loss = 0.04836687073111534
Training Iteration 15104: loss = 0.08529502153396606
Training Iteration 15360: loss = 0.05332143232226372
Training Iteration 15616: loss = 0.06140831857919693
Training Iteration 15872: loss = 0.05364150553941727
Training Iteration 16128: loss = 0.06246006488800049
Training Iteration 16384: loss = 0.05705783888697624
Training Iteration 16640: loss = 0.056011419743299484
Training Iteration 16896: loss = 0.04372896999120712
Training Iteration 17152: loss = 0.05473745986819267
Training Iteration 17408: loss = 0.054451536387205124
Training Iteration 17664: loss = 0.04592878371477127
Training Iteration 17920: loss = 0.041661519557237625
Training Iteration 18176: loss = 0.05266845226287842
Training Iteration 18432: loss = 0.04037504643201828
Training Iteration 18688: loss = 0.04601685330271721
Training Iteration 18944: loss = 0.04752279445528984
Training Iteration 19200: loss = 0.056916140019893646
Training Iteration 19456: loss = 0.061321597546339035
Training Iteration 19712: loss = 0.057276077568531036
Training Iteration 19968: loss = 0.06847216933965683
Training Iteration 20224: loss = 0.05685199424624443
Training Iteration 20480: loss = 0.03802981600165367
Training Iteration 20736: loss = 0.051331471651792526
Training Iteration 20992: loss = 0.06245332211256027
Training Iteration 21248: loss = 0.054299116134643555
Training Iteration 21504: loss = 0.049049943685531616
Training Iteration 21760: loss = 0.04188184067606926
Training Iteration 22016: loss = 0.06069397181272507
Training Iteration 22272: loss = 0.045529019087553024
Training Iteration 22528: loss = 0.05641820281744003
Training Iteration 22784: loss = 0.053694404661655426
Training Iteration 23040: loss = 0.04784185811877251
Training Iteration 23296: loss = 0.06101581081748009
Training Iteration 23552: loss = 0.056172095239162445
Training Iteration 23808: loss = 0.04813465103507042
Training Iteration 24064: loss = 0.05616191774606705
Training Iteration 24320: loss = 0.041160255670547485
Training Iteration 24576: loss = 0.05536694452166557
Training Iteration 24832: loss = 0.04657043516635895
Training Iteration 25088: loss = 0.06387162208557129
Training Iteration 25344: loss = 0.04810319095849991
Training Iteration 25600: loss = 0.045181866735219955
Training Iteration 25856: loss = 0.04323990270495415
Training Iteration 26112: loss = 0.05225582420825958
Training Iteration 26368: loss = 0.05245759338140488
Training Iteration 26624: loss = 0.04687836021184921
Training Iteration 26880: loss = 0.05174720287322998
Training Iteration 27136: loss = 0.04093605652451515
Training Iteration 27392: loss = 0.04731063172221184
Training Iteration 27648: loss = 0.042946361005306244
Training Iteration 27904: loss = 0.044562261551618576
Training Iteration 28160: loss = 0.045590147376060486
Training Iteration 28416: loss = 0.036081187427043915
Training Iteration 28672: loss = 0.05688773840665817
Training Iteration 28928: loss = 0.056631989777088165
Training Iteration 29184: loss = 0.05795111507177353
Training Iteration 29440: loss = 0.05433208867907524
Training Iteration 29696: loss = 0.047988783568143845
Training Iteration 29952: loss = 0.05257599800825119
Training Iteration 30208: loss = 0.04136784374713898
Training Iteration 30464: loss = 0.04615909606218338
Training Iteration 30720: loss = 0.04970957338809967
Training Iteration 30976: loss = 0.04674467444419861
Training Iteration 31232: loss = 0.05462314933538437
Training Iteration 31488: loss = 0.04228338226675987
Training Iteration 31744: loss = 0.042967259883880615
Training Iteration 32000: loss = 0.04764547944068909
Training Iteration 32256: loss = 0.04993073642253876
Training Iteration 32512: loss = 0.05042309686541557
Training Iteration 32768: loss = 0.05104352533817291
Training Iteration 33024: loss = 0.0486379973590374
Training Iteration 33280: loss = 0.047021232545375824
Training Iteration 33536: loss = 0.04457234591245651
Training Iteration 33792: loss = 0.04448263347148895
Training Iteration 34048: loss = 0.050670087337493896
Training Iteration 34304: loss = 0.031831204891204834
Training Iteration 34560: loss = 0.04661756753921509
Training Iteration 34816: loss = 0.036324940621852875
Training Iteration 35072: loss = 0.055987875908613205
Training Iteration 35328: loss = 0.027213338762521744
Training Iteration 35584: loss = 0.048495423048734665
Training Iteration 35840: loss = 0.04433970898389816
Training Iteration 36096: loss = 0.044644176959991455
Training Iteration 36352: loss = 0.056064024567604065
Training Iteration 36608: loss = 0.03398006781935692
Training Iteration 36864: loss = 0.04589002579450607
Training Iteration 37120: loss = 0.04159010574221611
Training Iteration 37376: loss = 0.05139092355966568
Training Iteration 37632: loss = 0.03970140963792801
Training Iteration 37888: loss = 0.0518866628408432
Training Iteration 38144: loss = 0.05120483785867691
Training Iteration 38400: loss = 0.05584287643432617
Training Iteration 38656: loss = 0.027050942182540894
Training Iteration 38912: loss = 0.03318373113870621
Training Iteration 39168: loss = 0.04071719944477081
Training Iteration 39424: loss = 0.05663110315799713
Training Iteration 39680: loss = 0.032609522342681885
Training Iteration 39936: loss = 0.045245930552482605
Training Iteration 40192: loss = 0.032968733459711075
Training Iteration 40448: loss = 0.03837897256016731
Training Iteration 40704: loss = 0.039020832628011703
Training Iteration 40960: loss = 0.05129663273692131
Training Iteration 41216: loss = 0.040947090834379196
Training Iteration 41472: loss = 0.034523896872997284
Training Iteration 41728: loss = 0.04349284991621971
Training Iteration 41984: loss = 0.04183351248502731
Training Iteration 42240: loss = 0.045593082904815674
Training Iteration 42496: loss = 0.04130729287862778
Training Iteration 42752: loss = 0.03491387888789177
Training Iteration 43008: loss = 0.042059581726789474
Training Iteration 43264: loss = 0.05399925634264946
Training Iteration 43520: loss = 0.04391723498702049
Training Iteration 43776: loss = 0.04086509719491005
Training Iteration 44032: loss = 0.040090739727020264
Training Iteration 44288: loss = 0.038119420409202576
Training Iteration 44544: loss = 0.043058402836322784
Training Iteration 44800: loss = 0.05204574018716812
Training Iteration 45056: loss = 0.049135275185108185
Training Iteration 45312: loss = 0.03835124894976616
Training Iteration 45568: loss = 0.04991079494357109
Training Iteration 45824: loss = 0.06832748651504517
Training Iteration 46080: loss = 0.04361535608768463
Training Iteration 46336: loss = 0.030597766861319542
Training Iteration 46592: loss = 0.03998464345932007
Training Iteration 46848: loss = 0.04964085668325424
Training Iteration 47104: loss = 0.040328480303287506
Training Iteration 47360: loss = 0.044465579092502594
Training Iteration 47616: loss = 0.028760876506567
Training Iteration 47872: loss = 0.04088716581463814
Training Iteration 48128: loss = 0.04575498029589653
Training Iteration 48384: loss = 0.030420538038015366
Training Iteration 48640: loss = 0.040518056601285934
Training Iteration 48896: loss = 0.06069362163543701
Training Iteration 49152: loss = 0.04492584988474846
Training Iteration 49408: loss = 0.040967926383018494
Training Iteration 49664: loss = 0.03784643113613129
Training Iteration 49920: loss = 0.03578152135014534
