  0%|                                                                                         | 0/50000 [00:00<?, ?it/s]/home/kaust/anaconda3/envs/drl/lib/python3.10/site-packages/torch/nn/functional.py:4999: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.
  warnings.warn(
Training Iteration 0: loss = 3.3042140007019043
 16%|████████████▊                                                                 | 8191/50000 [02:28<12:38, 55.10it/s]
Training Iteration 256: loss = 0.3065878748893738
Training Iteration 512: loss = 0.20415374636650085
Training Iteration 768: loss = 0.17299668490886688
Training Iteration 1024: loss = 0.1415361762046814
Training Iteration 1280: loss = 0.12133395671844482
Training Iteration 1536: loss = 0.12071774899959564
Training Iteration 1792: loss = 0.1202978640794754
Training Iteration 2048: loss = 0.10905493795871735
Training Iteration 2304: loss = 0.09665770828723907
Training Iteration 2560: loss = 0.087479367852211
Training Iteration 2816: loss = 0.09519684314727783
Training Iteration 3072: loss = 0.08799847960472107
Training Iteration 3328: loss = 0.10126708447933197
Training Iteration 3584: loss = 0.10007920116186142
Training Iteration 3840: loss = 0.094191774725914
Training Iteration 4096: loss = 0.09590619802474976
Training Iteration 4352: loss = 0.07957033812999725
Training Iteration 4608: loss = 0.10149727761745453
Training Iteration 4864: loss = 0.07465720176696777
Training Iteration 5120: loss = 0.09334039688110352
Training Iteration 5376: loss = 0.09209287166595459
Training Iteration 5632: loss = 0.08029739558696747
Training Iteration 5888: loss = 0.07954466342926025
Training Iteration 6144: loss = 0.07560262084007263
Training Iteration 6400: loss = 0.06603386998176575
Training Iteration 6656: loss = 0.07991810142993927
Training Iteration 6912: loss = 0.08280631899833679
Training Iteration 7168: loss = 0.06954239308834076
Training Iteration 7424: loss = 0.08369895815849304
Training Iteration 7680: loss = 0.0721139907836914
Training Iteration 7936: loss = 0.05806697905063629
Training Iteration 8192: loss = 0.06616514921188354
Training Iteration 8448: loss = 0.07771492004394531
Training Iteration 8704: loss = 0.07676347345113754
Training Iteration 8960: loss = 0.06247745081782341
Training Iteration 9216: loss = 0.05617532134056091
Training Iteration 9472: loss = 0.07518421113491058
Training Iteration 9728: loss = 0.07829148322343826
Training Iteration 9984: loss = 0.05681435391306877
Training Iteration 10240: loss = 0.05780106037855148
Training Iteration 10496: loss = 0.040151216089725494
Training Iteration 10752: loss = 0.08231513202190399
Training Iteration 11008: loss = 0.05863457918167114
Training Iteration 11264: loss = 0.07575888931751251
Training Iteration 11520: loss = 0.051119767129421234
Training Iteration 11776: loss = 0.060673605650663376
Training Iteration 12032: loss = 0.05961295962333679
Training Iteration 12288: loss = 0.0694786086678505
Training Iteration 12544: loss = 0.0523291639983654
Training Iteration 12800: loss = 0.07334183901548386
Training Iteration 13056: loss = 0.072247713804245
Training Iteration 13312: loss = 0.0656726062297821
Training Iteration 13568: loss = 0.0680057555437088
Training Iteration 13824: loss = 0.04821565002202988
Training Iteration 14080: loss = 0.049289047718048096
Training Iteration 14336: loss = 0.05906463786959648
Training Iteration 14592: loss = 0.054174911230802536
Training Iteration 14848: loss = 0.0615043044090271
Training Iteration 15104: loss = 0.08139526098966599
Training Iteration 15360: loss = 0.04764656722545624
Training Iteration 15616: loss = 0.06667092442512512
Training Iteration 15872: loss = 0.05836344510316849
Training Iteration 16128: loss = 0.05454174801707268
Training Iteration 16384: loss = 0.06737805157899857
Training Iteration 16640: loss = 0.050136834383010864
Training Iteration 16896: loss = 0.06217556446790695
Training Iteration 17152: loss = 0.04978496953845024
Training Iteration 17408: loss = 0.04532564431428909
Training Iteration 17664: loss = 0.061601173132658005
Training Iteration 17920: loss = 0.04399026557803154
Training Iteration 18176: loss = 0.061882928013801575
Training Iteration 18432: loss = 0.06250019371509552
Training Iteration 18688: loss = 0.05460871011018753
Training Iteration 18944: loss = 0.05052970349788666
Training Iteration 19200: loss = 0.053789615631103516
Training Iteration 19456: loss = 0.046263448894023895
Training Iteration 19712: loss = 0.04801363870501518
Training Iteration 19968: loss = 0.04755283519625664
Training Iteration 20224: loss = 0.05457824468612671
Training Iteration 20480: loss = 0.0646640881896019
Training Iteration 20736: loss = 0.04632807895541191
Training Iteration 20992: loss = 0.04957987368106842
Training Iteration 21248: loss = 0.05599868297576904
Training Iteration 21504: loss = 0.03556305915117264
Training Iteration 21760: loss = 0.058804433792829514
Training Iteration 22016: loss = 0.05431317538022995
Training Iteration 22272: loss = 0.042183198034763336
Training Iteration 22528: loss = 0.06811083853244781
Training Iteration 22784: loss = 0.04175766557455063
Training Iteration 23040: loss = 0.05563659965991974
Training Iteration 23296: loss = 0.0451718270778656
Training Iteration 23552: loss = 0.043295785784721375
Training Iteration 23808: loss = 0.052050571888685226
Training Iteration 24064: loss = 0.04653279110789299
Training Iteration 24320: loss = 0.05445871874690056
Training Iteration 24576: loss = 0.04511883482336998
Training Iteration 24832: loss = 0.049202144145965576
Training Iteration 25088: loss = 0.056996241211891174
Training Iteration 25344: loss = 0.05213146656751633
Training Iteration 25600: loss = 0.04882856458425522
Training Iteration 25856: loss = 0.048502981662750244
Training Iteration 26112: loss = 0.04820990562438965
Training Iteration 26368: loss = 0.043245282024145126
Training Iteration 26624: loss = 0.05361465737223625
Training Iteration 26880: loss = 0.05520512908697128
Training Iteration 27136: loss = 0.045946210622787476
Training Iteration 27392: loss = 0.038577258586883545
Training Iteration 27648: loss = 0.049964144825935364
Training Iteration 27904: loss = 0.06111779808998108
Training Iteration 28160: loss = 0.04424768686294556
Training Iteration 28416: loss = 0.041040144860744476
Training Iteration 28672: loss = 0.045705463737249374
Training Iteration 28928: loss = 0.05463629961013794
Training Iteration 29184: loss = 0.04379122704267502
Training Iteration 29440: loss = 0.03836066275835037
Training Iteration 29696: loss = 0.04724843427538872
Training Iteration 29952: loss = 0.04438069462776184
Training Iteration 30208: loss = 0.05582376569509506
Training Iteration 30464: loss = 0.05024603009223938
Training Iteration 30720: loss = 0.056955479085445404
Training Iteration 30976: loss = 0.04795412719249725
Training Iteration 31232: loss = 0.036808546632528305
Training Iteration 31488: loss = 0.0377197340130806
Training Iteration 31744: loss = 0.06517438590526581
Training Iteration 32000: loss = 0.051185399293899536
Training Iteration 32256: loss = 0.0537397675216198
Training Iteration 32512: loss = 0.055845826864242554
Training Iteration 32768: loss = 0.05255730077624321
Training Iteration 33024: loss = 0.03949138522148132
Training Iteration 33280: loss = 0.037465404719114304
Training Iteration 33536: loss = 0.04867539927363396
Training Iteration 33792: loss = 0.0511174201965332
Training Iteration 34048: loss = 0.04698937386274338
Training Iteration 34304: loss = 0.04634885489940643
Training Iteration 34560: loss = 0.04761573672294617
Training Iteration 34816: loss = 0.04092298448085785
Training Iteration 35072: loss = 0.040647443383932114
Training Iteration 35328: loss = 0.05179869756102562
Training Iteration 35584: loss = 0.04576381668448448
Training Iteration 35840: loss = 0.04357852786779404
Training Iteration 36096: loss = 0.03798101842403412
Training Iteration 36352: loss = 0.0413212776184082
Training Iteration 36608: loss = 0.04730610176920891
Training Iteration 36864: loss = 0.04230522736907005
Training Iteration 37120: loss = 0.0466228723526001
Training Iteration 37376: loss = 0.04644237086176872
Training Iteration 37632: loss = 0.04184722155332565
Training Iteration 37888: loss = 0.03628856688737869
Training Iteration 38144: loss = 0.03854338824748993
Training Iteration 38400: loss = 0.04165935143828392
Training Iteration 38656: loss = 0.04215272516012192
Training Iteration 38912: loss = 0.05357421562075615
Training Iteration 39168: loss = 0.04355219006538391
Training Iteration 39424: loss = 0.04694592207670212
Training Iteration 39680: loss = 0.035684891045093536
Training Iteration 39936: loss = 0.0562700480222702
Training Iteration 40192: loss = 0.038437359035015106
Training Iteration 40448: loss = 0.05071026086807251
Training Iteration 40704: loss = 0.031237926334142685
Training Iteration 40960: loss = 0.03585905209183693
Training Iteration 41216: loss = 0.03753285109996796
Training Iteration 41472: loss = 0.04033539816737175
Training Iteration 41728: loss = 0.029211610555648804
Training Iteration 41984: loss = 0.04425102099776268
Training Iteration 42240: loss = 0.05072822794318199
Training Iteration 42496: loss = 0.03798414766788483
Training Iteration 42752: loss = 0.03444555401802063
Training Iteration 43008: loss = 0.03883901238441467
Training Iteration 43264: loss = 0.0293903611600399
Training Iteration 43520: loss = 0.03174838051199913
Training Iteration 43776: loss = 0.04190364480018616
Training Iteration 44032: loss = 0.03961719945073128
Training Iteration 44288: loss = 0.05218035727739334
Training Iteration 44544: loss = 0.051101021468639374
Training Iteration 44800: loss = 0.05229294300079346
Training Iteration 45056: loss = 0.03884739801287651
Training Iteration 45312: loss = 0.05680958554148674
Training Iteration 45568: loss = 0.04418466240167618
Training Iteration 45824: loss = 0.044500671327114105
Training Iteration 46080: loss = 0.029109980911016464
Training Iteration 46336: loss = 0.03609978407621384
Training Iteration 46592: loss = 0.0455218069255352
Training Iteration 46848: loss = 0.04574115201830864
Training Iteration 47104: loss = 0.05033118650317192
Training Iteration 47360: loss = 0.04506271332502365
Training Iteration 47616: loss = 0.04554902762174606
Training Iteration 47872: loss = 0.04346116632223129
Training Iteration 48128: loss = 0.038721513003110886
Training Iteration 48384: loss = 0.04363560676574707
Training Iteration 48640: loss = 0.040479887276887894
Training Iteration 48896: loss = 0.03800872340798378
Training Iteration 49152: loss = 0.029549499973654747
Training Iteration 49408: loss = 0.0352296456694603
Training Iteration 49664: loss = 0.041269153356552124
Training Iteration 49920: loss = 0.039731141179800034
